file.edit("~/.Rprofile")
blogdown::serve_site()
x <- c("tidyverse", "readr", "rvest", "openxlsx")
lapply(x, require, character.only = TRUE, quietly = T,  warn.conflicts = F)
scraping <- function(url, css){
html_text(html_nodes(read_html(url), css))
}
url_issue <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/issue/pool"
url_argument <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/argument/pool"
## store issue and argument page as "issue" and "argument", respectively
issue <- scraping(url_issue, ".divider-50~ p , .indented p")
argument <- scraping(url_argument, ".divider-50~ p , .indented p")
getwd()
saveWorkbook(wb, file = "./static/GRE_AW.xlsx", overwrite = TRUE)
## For issue, find the index with sentense start with "claim"
start_claim <- unname(which(sapply(issue, substr, start = 1, stop = 5 ) == "Claim"))
## paste start with "claim" item with each of the following items (started with "Reason")
issue[start_claim] <- paste(issue[start_claim], "\n",issue[start_claim+1])
## delete item start with "Reason"
issue <- issue[-(start_claim + 1)]
## now odd is prompt and even is question
issue_tbl <- tbl_df(matrix(issue, ncol = 2, byrow = T)) %>%
mutate(., qn_no = as.numeric(factor(V2))) # number the question
colnames(issue_tbl) <- c("background_info", "question", "qn_no")
######################################################################
## for argument, double-line promt has no single/simple pattern, we identied/indexed questions.
start_response <- unname(which(sapply(argument, substr, start = 1, stop = 16) == "Write a response"))
## creating interval between each question to next.
interval <- findInterval(1:493, start_response, rightmost.closed = T, left.open = T)
interval[2] <- c(0) ## a minor adjust of the first interval.
## split the prompts out, aggregate paste neighbour with sample interval number.
interval_tbl <- tbl_df(cbind(argument, interval)) %>%
group_by(., interval) %>%
filter(., row_number() != n()) %>%
ungroup()
bg_tbl <- aggregate(argument~interval, interval_tbl, paste, collapse = "\n") %>%
tbl_df
## split the questions out.
qn_tbl <- tbl_df(cbind(argument, interval)) %>%
group_by(., interval) %>%
filter(., row_number() == n()) %>%
ungroup()
## joining prompts and questions.
argument_tbl <- inner_join(bg_tbl, qn_tbl, by = c("interval" = "interval")) %>%
select(., -interval) %>%
mutate(., qn_no = as.numeric(factor(argument.y)))
colnames(argument_tbl) <- c("background_info", "question", "qn_no")
######################################################################
## writing the date out
wb <- createWorkbook()
sn <- "argument"
addWorksheet(wb = wb, sheetName = sn)
writeData(wb = wb, sheet = sn, x = argument_tbl, borders = "n")
sn <- "issue"
addWorksheet(wb = wb, sheetName = sn)
writeData(wb = wb, sheet = sn, x = issue_tbl, borders = "n")
saveWorkbook(wb, file = "./static/GRE_AW.xlsx", overwrite = TRUE)
x <- c("tidyverse", "readr", "rvest", "openxlsx", "kable", "kable_extra")
lapply(x, require, character.only = TRUE, quietly = T,  warn.conflicts = F)
scraping <- function(url, css){
html_text(html_nodes(read_html(url), css))
}
url_issue <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/issue/pool"
url_argument <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/argument/pool"
## store issue and argument page as "issue" and "argument", respectively
issue <- scraping(url_issue, ".divider-50~ p , .indented p")
argument <- scraping(url_argument, ".divider-50~ p , .indented p")
x <- c("tidyverse", "readr", "rvest", "openxlsx", "kable", "kable_extra")
lapply(x, require, character.only = TRUE, quietly = T,  warn.conflicts = F)
scraping <- function(url, css){
html_text(html_nodes(read_html(url), css))
}
url_issue <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/issue/pool"
url_argument <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/argument/pool"
## store issue and argument page as "issue" and "argument", respectively
issue <- scraping(url_issue, ".divider-50~ p , .indented p")
argument <- scraping(url_argument, ".divider-50~ p , .indented p")
x <- c("tidyverse", "readr", "rvest", "openxlsx", "kable", "kable_extra")
lapply(x, require, character.only = TRUE, quietly = T,  warn.conflicts = F)
scraping <- function(url, css){
html_text(html_nodes(read_html(url), css))
}
url_issue <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/issue/pool"
url_argument <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/argument/pool"
## store issue and argument page as "issue" and "argument", respectively
issue <- scraping(url_issue, ".divider-50~ p , .indented p")
argument <- scraping(url_argument, ".divider-50~ p , .indented p")
x <- c("tidyverse", "readr", "rvest", "openxlsx", "kable", "kable_extra")
lapply(x, require, character.only = TRUE, quietly = T,  warn.conflicts = F)
scraping <- function(url, css){
html_text(html_nodes(read_html(url), css))
}
url_issue <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/issue/pool"
url_argument <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/argument/pool"
## store issue and argument page as "issue" and "argument", respectively
issue <- scraping(url_issue, ".divider-50~ p , .indented p")
argument <- scraping(url_argument, ".divider-50~ p , .indented p")
## kable output
kable(argument_tbl, "html") %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
scroll_box(width = "500px", height = "200px")
x <- c("tidyverse", "readr", "rvest", "openxlsx", "kable", "kable_extra")
lapply(x, require, character.only = TRUE, quietly = T,  warn.conflicts = F)
scraping <- function(url, css){
html_text(html_nodes(read_html(url), css))
}
url_issue <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/issue/pool"
url_argument <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/argument/pool"
## store issue and argument page as "issue" and "argument", respectively
issue <- scraping(url_issue, ".divider-50~ p , .indented p")
argument <- scraping(url_argument, ".divider-50~ p , .indented p")
x <- c("tidyverse", "readr", "rvest", "openxlsx", "kableExtra")
lapply(x, require, character.only = TRUE, quietly = T,  warn.conflicts = F)
scraping <- function(url, css){
html_text(html_nodes(read_html(url), css))
}
url_issue <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/issue/pool"
url_argument <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/argument/pool"
## store issue and argument page as "issue" and "argument", respectively
issue <- scraping(url_issue, ".divider-50~ p , .indented p")
argument <- scraping(url_argument, ".divider-50~ p , .indented p")
x <- c("tidyverse", "readr", "rvest", "openxlsx", "kableExtra")
lapply(x, require, character.only = TRUE, quietly = T,  warn.conflicts = F)
scraping <- function(url, css){
html_text(html_nodes(read_html(url), css))
}
url_issue <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/issue/pool"
url_argument <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/argument/pool"
## store issue and argument page as "issue" and "argument", respectively
issue <- scraping(url_issue, ".divider-50~ p , .indented p")
argument <- scraping(url_argument, ".divider-50~ p , .indented p")
css <- "tbody td{
padding: 30px;
}
tbody tr:nth-child(odd){
background-color: #4C8BF5;
color: #fff;
}"
## kable output
kable(argument_tbl, "html") %>%
kable_styling(bootstrap_options = c("striped", "hover"), font_size = 7) %>%
scroll_box(width = "700px", height = "200px", extra_css = css)
blogdown:::serve_site()
scroll_box()
scroll_box()
kableExtra::scroll_box()
kableExtra::scroll_box
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
x <- c("tidyverse", "survival", "coxme", "penalized", "globaltest", "knitr", "kableExtra", "vegan")
lapply(x, require, character.only = TRUE, quietly = T,  warn.conflicts = F)
options(contrasts=c("contr.treatment","contr.treatment"))
tma <- read_csv("/Users/Guoshiyuan/Desktop/RA_admin/org_anat/master_study/BL5233/final_project/dataset/gene_m_TMA.csv")
tma$slide_number <- factor(tma$slide_number) # unordered factor
tma$race <- factor(tma$race) # unordered factor
tma$BR_grade <- factor(tma$BR_grade, ordered=TRUE) # ordered factor
tma$BR_score <- factor(tma$BR_score, ordered=TRUE) # ordered factor
tma$LVI <-  tma$LVI == "present"# logical
tma$LN_stage <- factor(tma$LN_stage, ordered=TRUE) # ordered
tma$ER <- tma$ER == "Positive"
tma$PR <- tma$PR == "Positive"
tma$HER2 <- tma$HER2 == "Positive"
tma$scoring_batch <- factor(tma$scoring_batch) # unordered factor
os <- coxme(Surv(OS,Mortality)~c_IRS+(1|age)+(1|race)+(1|size)+(1|BR_grade)+(1|LVI), data = tma)
os
os <- coxme(Surv(OS,Mortality)~c_IRS+(1|age)+(1|race)+(1|size)+(1|BR_grade)+(1|LVI)+(1|LN_stage), data = tma)
blogdown:::serve_site()
library(leaflet)
```{r leaflet, fig.cap= "Places I have being", out.width='100%', fig.asp=0.5}
library(leaflet)
m <- leaflet() %>%
addTiles() %>%  # Add default OpenStreetMap map tiles
addMarkers(lng=-1.39595, lat=50.93463, popup="Southampton University")
m
```
.libPaths()
.libPaths()
update.packages(ask=FALSE, checkBuilt=TRUE)
rvcheck::update_all()
install.packages(c("BayesXsrc", "clue", "futile.options", "gamlss", "gamlss.data", "gamlss.dist", "htmlwidgets", "httpuv", "kernlab", "lambda.r", "leaflet", "leaflet.minicharts", "ordinal", "party", "pillar", "prodlim", "ProjectTemplate", "R.oo", "RcppArmadillo", "readxl", "robustbase", "Rvmmin", "slam", "sourcetools", "spBayesSurv", "survival", "tinytex", "vegan", "XML"))
library("yaml", lib.loc="~/Library/R/3.5/library")
remove.packages("yaml", lib="~/Library/R/3.5/library")
install.packages("yaml")
install.packages(c("BayesXsrc", "clue", "futile.options", "gamlss", "gamlss.data", "gamlss.dist", "htmlwidgets", "httpuv", "kernlab", "lambda.r", "leaflet", "leaflet.minicharts", "ordinal", "party", "pillar", "prodlim", "ProjectTemplate", "R.oo", "RcppArmadillo", "readxl", "robustbase", "Rvmmin", "slam", "sourcetools", "spBayesSurv", "survival", "tinytex", "vegan", "XML"))
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
if (!require(tidykml)){
library(devtools); install_github("briatte/tidykml")
if(!require(tidykml)) stop("sth wrong with tidykml")
}
library(leaflet)
kml_read("../static/the_places_I_have_been.kml") %>%
kml_points(.) %>%
leaflet(.) %>%
addTiles() %>%
setView(20,20,1.3) %>%
addCircles(lng=~longitude, lat=~latitude, popup = ~name, radius=40, stroke = TRUE, fillOpacity = 0.8)
getwd()
if (!require(tidykml)){
library(devtools); install_github("briatte/tidykml")
if(!require(tidykml)) stop("sth wrong with tidykml")
}
library(leaflet)
kml_read("/static/the_places_I_have_been.kml") %>%
kml_points(.) %>%
leaflet(.) %>%
addTiles() %>%
setView(20,20,1.3) %>%
addCircles(lng=~longitude, lat=~latitude, popup = ~name, radius=40, stroke = TRUE, fillOpacity = 0.8)
if (!require(tidykml)){
library(devtools); install_github("briatte/tidykml")
if(!require(tidykml)) stop("sth wrong with tidykml")
}
library(leaflet)
kml_read("/the_places_I_have_been.kml") %>%
kml_points(.) %>%
leaflet(.) %>%
addTiles() %>%
setView(20,20,1.3) %>%
addCircles(lng=~longitude, lat=~latitude, popup = ~name, radius=40, stroke = TRUE, fillOpacity = 0.8)
blogdown:::serve_site()
if (!require(tidykml)){
library(devtools); install_github("briatte/tidykml")
if(!require(tidykml)) stop("sth wrong with tidykml")
}
library(leaflet)
kml_read("/static/the_places_I_have_been.kml") %>%
kml_points(.) %>%
leaflet(.) %>%
addTiles() %>%
setView(20,20,1.3) %>%
addCircles(lng=~longitude, lat=~latitude, popup = ~name, radius=40, stroke = TRUE, fillOpacity = 0.8)
if (!require(tidykml)){
library(devtools); install_github("briatte/tidykml")
if(!require(tidykml)) stop("sth wrong with tidykml")
}
library(leaflet)
kml_read("./static/the_places_I_have_been.kml") %>%
kml_points(.) %>%
leaflet(.) %>%
addTiles() %>%
setView(20,20,1.3) %>%
addCircles(lng=~longitude, lat=~latitude, popup = ~name, radius=40, stroke = TRUE, fillOpacity = 0.8)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
