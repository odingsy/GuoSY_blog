---
title: Applying web scrapping technics in collecting GRE analytical writing topics
author: Guo Shiyuan
date: '2017-12-02'
slug: applying-web-scrapping-technics-in-collecting-gre-analytical-writing-topics
categories:
  - R
tags: [collect_info]
---

# Introduction to Web scrapping in R 
Web scrapping/crawling takes the advantages of common html tags to extract information from a webpage. It is useful in collecting and organising structured information for human or passed to further workflow. 

In R platform, `rvest` is the package that includes functions to read html source code by providing page urls; with the help from a css selector as a chrome plugin [SelectorGadget](http://selectorgadget.com/), which identifies tags of interest, web scrapping can be realised without html/css knowledge. 

The main source I learnt this technique are as follows. 

* [scrapping intro by analyticsvidhya](https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/)
* [rvest tutorial: scraping the web using R](https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/)
* vignette("selectorgadget") under `rvest`

# understanding the data
The scrapping subject is from question pools of GRE Analytical Writing. There are two pools, [issue](https://www.ets.org/gre/revised_general/prepare/analytical_writing/issue/pool) and [arguement](https://www.ets.org/gre/revised_general/prepare/analytical_writing/argument/pool), with the format varies.

* Issue Pool is a list of 152. It starts with a prompt for contextualising to a specific topic and a task for providing a direction. Task portion contains 6 potential options and it always starts with "write a response in which you discuss...". Prompt portion varies in format and number of sentences; two-sentence prompt are claim-response type, which the two start with "claim" and "response" respectively. 
* Argument Pool contains 176 entities. It also gives a background followed by task. The background varies while there are only 8 tasks which start with same "write a response in which you discuss...". There are also two-sentence prompt and the first sentence is always "the following...". 


# Approach 
The strategy of scrapping this information from the web page and split them into prompt Vs task is as follows

1. from html web address, extract the individual entities by using html tags `.divider-50~ p , .indented p` (assisted by SelectorGadget). 

```{r initialtion, message=FALSE, warning=FALSE, include=FALSE}
x <- c("tidyverse", "readr", "rvest", "openxlsx", "kableExtra")
lapply(x, require, character.only = TRUE, quietly = T,  warn.conflicts = F)

scraping <- function(url, css){
    html_text(html_nodes(read_html(url), css))
}

url_issue <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/issue/pool"
url_argument <- "https://www.ets.org/gre/revised_general/prepare/analytical_writing/argument/pool"

## store issue and argument page as "issue" and "argument", respectively
issue <- scraping(url_issue, ".divider-50~ p , .indented p") 
argument <- scraping(url_argument, ".divider-50~ p , .indented p")
```



2. To organised the scrapped information into prompt and question, I processed the captured information in 
```{r}
## For issue, find the index with sentense start with "claim"
start_claim <- unname(which(sapply(issue, substr, start = 1, stop = 5 ) == "Claim"))
## paste start with "claim" item with each of the following items (started with "Reason")
issue[start_claim] <- paste(issue[start_claim], "\n",issue[start_claim+1]) 
## delete item start with "Reason"
issue <- issue[-(start_claim + 1)]
## now odd is prompt and even is question 
issue_tbl <- tbl_df(matrix(issue, ncol = 2, byrow = T)) %>%
    mutate(., qn_no = as.numeric(factor(V2))) # number the question 
colnames(issue_tbl) <- c("background_info", "question", "qn_no")

######################################################################
## for argument, double-line promt has no single/simple pattern, we identied/indexed questions. 
start_response <- unname(which(sapply(argument, substr, start = 1, stop = 16) == "Write a response"))
## creating interval between each question to next. 
interval <- findInterval(1:493, start_response, rightmost.closed = T, left.open = T)
interval[2] <- c(0) ## a minor adjust of the first interval. 
## split the prompts out, aggregate paste neighbour with sample interval number. 
interval_tbl <- tbl_df(cbind(argument, interval)) %>%
    group_by(., interval) %>%
    filter(., row_number() != n()) %>%
    ungroup()
bg_tbl <- aggregate(argument~interval, interval_tbl, paste, collapse = "\n") %>%
    tbl_df
## split the questions out. 
qn_tbl <- tbl_df(cbind(argument, interval)) %>%
    group_by(., interval) %>%
    filter(., row_number() == n()) %>%
    ungroup()
## joining prompts and questions. 
argument_tbl <- inner_join(bg_tbl, qn_tbl, by = c("interval" = "interval")) %>%
    select(., -interval) %>%
    mutate(., qn_no = as.numeric(factor(argument.y)))
colnames(argument_tbl) <- c("background_info", "question", "qn_no")

######################################################################
## writing in excel
# wb <- createWorkbook()
# 
# sn <- "argument"
# addWorksheet(wb = wb, sheetName = sn)
# writeData(wb = wb, sheet = sn, x = argument_tbl, borders = "n")
# 
# sn <- "issue"
# addWorksheet(wb = wb, sheetName = sn)
# 
# writeData(wb = wb, sheet = sn, x = issue_tbl, borders = "n")
# saveWorkbook(wb, file = "./static/GRE_AW.xlsx", overwrite = TRUE)
```

```{r echo=FALSE}
css <- "tbody tr:nth-child(odd){
  background-color: #4C8BF5;
  color: #fff;
} "
## kable output 
kable(argument_tbl, "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), font_size = 7) %>%
  scroll_box(width = "700px", height = "200px", extra_css = css)
```

```{r echo=FALSE}
## kable output 
kable(issue_tbl, "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), font_size = 7) %>%
  scroll_box(width = "700px", height = "200px", extra_css = css)
```

# [Output](/GRE_AW.xlsx) and reflection
Capturing dataset was trivial while organising it requires a fair understand about its structure. The idea of organising issue and argument was conceived in a chronological order, so that the way to organise argument was more generalisable because it use a shared feature in the question instead of the feature only appear in each type. Although no benchmarking on code efficiency was not performed and no guarantee of the most efficient/elegant way of doing, this post proposed a context for web scrapping to solve problem and presented some challenges with data organisation. 


